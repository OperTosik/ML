{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04352d0-1706-4d50-b038-396e3ea514fb",
   "metadata": {},
   "source": [
    "Link to data: https://www.kaggle.com/code/gusthema/house-prices-prediction-using-tfdf/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "4766f865-603a-4a09-8429-7ddfe5f0322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import time\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a41c2-d764-40fc-bd4b-583ae1958252",
   "metadata": {},
   "source": [
    "# **Считывание данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "2eccb906-4c3d-47ec-8d0a-d34e00d313c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>169277.052498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>187758.393989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>183583.683570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>179317.477511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>150730.079977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>167081.220949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>164788.778231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>219222.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>184924.279659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>187741.866657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  169277.052498\n",
       "1     1462  187758.393989\n",
       "2     1463  183583.683570\n",
       "3     1464  179317.477511\n",
       "4     1465  150730.079977\n",
       "...    ...            ...\n",
       "1454  2915  167081.220949\n",
       "1455  2916  164788.778231\n",
       "1456  2917  219222.423400\n",
       "1457  2918  184924.279659\n",
       "1458  2919  187741.866657\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('dataset/sample_submission.csv')\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f44f61-6895-4fbc-8a28-42b2291b3a02",
   "metadata": {},
   "source": [
    "Сразу можно сказать, что тип задачи: регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423aefe9-868d-4646-ac23-effc4c896312",
   "metadata": {},
   "source": [
    "Файл ниже является переводом файла dataset/data_describtion.txt\n",
    "Он лежит на гите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "66a8be3f-0704-42a2-a064-08bb8c8a0b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSSubClass: Классификация типа жилого объекта\n",
      "\n",
      "20 — Одноэтажный, 1946 г. и новее (все стили)\n",
      "30 — Одноэтажный, 1945 г. и старше\n",
      "40 — Одноэтажный с готовой мансардой\n",
      "45 — Полутораэтажный с незавершённой отделкой\n",
      "50 — Полутораэтажный с завершённой отделкой\n",
      "60 — Двухэтажный, 1946+\n",
      "70 — Двухэтажный, 1945 и старше\n",
      "75 — Два с половиной этажа\n",
      "80 — Раздельный/многоуровневый\n",
      "85 — Дом с разделённым холлом\n",
      "90 — Дуплекс\n",
      "120 — Одноэтажный PUD (квартальная застройка), 1946+\n",
      "150 — Полутораэтажный PUD\n",
      "160 — Двухэтажный PUD, 1946+\n",
      "180 — Многоуровневый PUD\n",
      "190 — Переделанный под две семьи\n",
      "MSZoning: Зонирование\n",
      "\n",
      "A — Сельскохозяйственное\n",
      "C — Коммерческое\n",
      "FV — Жилой комплекс у воды\n",
      "I — Промышленное\n",
      "RH — Жилая зона высокой плотности\n",
      "RL — Жилая зона низкой плотности\n",
      "RP — Жилая зона с парком\n",
      "RM — Жилая зона средней плотности\n",
      "Основные параметры участка:\n",
      "\n",
      "LotFrontage — Примыкание к улице (линейные футы)\n",
      "LotArea — Площадь участка (кв. футы)\n",
      "Street — Тип дороги:\n",
      "Grvl — Грунтовая\n",
      "Pave — Асфальтированная\n",
      "Alley — Переулок:\n",
      "Grvl/Pave/NA (нет доступа)\n",
      "Характеристики участка:\n",
      "\n",
      "LotShape — Форма:\n",
      "Reg (правильная), IR1-IR3 (степень неправильности)\n",
      "LandContour — Рельеф:\n",
      "Lvl (ровный), Bnk (склон), HLS (холмистый), Low (низина)\n",
      "Utilities — Коммуникации:\n",
      "AllPub (полные), NoSewr (без канализации), ELO (только электричество)\n",
      "LandSlope — Уклон:\n",
      "Gtl (пологий), Mod (умеренный), Sev (крутой)\n",
      "Расположение:\n",
      "\n",
      "Neighborhood — 28 районов г. Эймс (например, OldTown — Старый город)\n",
      "Condition1/2 — Близость к:\n",
      "Artery (магистрали), RR (жд-пути), PosN (парки)\n",
      "Характеристики здания:\n",
      "\n",
      "BldgType — Тип:\n",
      "1Fam (отдельный дом), TwnhsE (таунхаус)\n",
      "HouseStyle — Архитектура:\n",
      "1Story (одноэтажный), 2.5Unf (2.5 этажа без отделки)\n",
      "OverallQual/Cond — Общее качество/состояние (шкала 1–10)\n",
      "Конструктивные элементы:\n",
      "\n",
      "RoofStyle — Тип крыши:\n",
      "Gable (двускатная), Hip (вальмовая)\n",
      "Foundation — Фундамент:\n",
      "PConc (монолитный бетон), CBlock (блочный)\n",
      "Exterior — Отделка:\n",
      "VinylSd (виниловый сайдинг), BrkFace (облицовочный кирпич)\n",
      "Подвал (Bsmt):\n",
      "\n",
      "Qual — Высота:\n",
      "Ex (100+ дюймов), NA (нет подвала)\n",
      "FinType — Отделка:\n",
      "GLQ (качественная), Unf (незавершённая)\n",
      "Коммуникации:\n",
      "\n",
      "Heating — Отопление:\n",
      "GasA (газовое), Grav (гравитационное)\n",
      "CentralAir — Кондиционирование:\n",
      "Y/N (да/нет)\n",
      "Electrical — Электрика:\n",
      "SBrkr (автоматы), FuseP (пробки)\n",
      "Площади:\n",
      "\n",
      "GrLivArea — Жилая площадь (кв. футы)\n",
      "GarageArea — Площадь гаража\n",
      "WoodDeckSF — Деревянный настил\n",
      "Продажа:\n",
      "\n",
      "SaleType — Тип сделки:\n",
      "WD (стандартная), New (новостройка)\n",
      "SaleCondition — Условия:\n",
      "Normal (обычная), Abnorml (форс-мажор)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat dataset/data_help.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "28c758cf-d7d9-47e5-a86a-b65f92d44ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузка данных\n",
    "X_train = pd.read_csv('dataset/train.csv')\n",
    "X_test = pd.read_csv('dataset/test.csv')\n",
    "y_train = X_train['SalePrice']\n",
    "\n",
    "#Подготовка y_test\n",
    "X_test['SalePrice'] = 0.\n",
    "y_test = sample['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "9659d0d5-5f9e-4080-ad05-31c49b3ded54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     588 non-null    object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "ee935f07-66db-42fe-af79-5c10c93a4cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5367599-581d-4637-86d2-8b3e7216198d",
   "metadata": {},
   "source": [
    "# *Обработка пропусков*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb099d3-04dd-4aad-87f9-321694a02b66",
   "metadata": {},
   "source": [
    "Мысль состоит в том, что если столбец имеет больше чем coeff*len(X_train) пропусков, то его надо удалять.\n",
    "В отсальных же случаях просто заменим пропуски на медианное значение.\n",
    "Также столюцы имеют типы данных либо int64, либо float64, либо Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "2c1eb4ea-8dc8-42d5-9b5c-66c9ae0c4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "97f7d1fb-b332-42c0-8457-8e746fbbbeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configNa(X, coeff):\n",
    "    for col in X.columns.tolist():\n",
    "        quantity_NaN = X[col].isnull().sum()\n",
    "        if (quantity_NaN >= coeff*len(X)):\n",
    "            X.drop(col, axis=1, inplace=True)\n",
    "        elif (quantity_NaN):\n",
    "            if (X[col].dtype != 'O'):\n",
    "                median = X[col].median()\n",
    "            else:\n",
    "                median = X[col].mode().values[0] #криво, знаю\n",
    "            X[col].fillna(value = median, inplace=True)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "74c91a9e-dbb6-4dd6-8562-20b56d0eb40f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               0\n",
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 74, dtype: int64"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configNa(X_train, coeff)\n",
    "configNa(X_test, coeff)\n",
    "X_train.isnull().sum() #Зато теперь нет пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "a87fed94-7eee-4940-bc06-c8b21c4123e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1343d5-55b3-45a3-85cc-906061d1b9ee",
   "metadata": {},
   "source": [
    "# **Обработка категориальных признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "514ea80b-0383-47d9-a0cc-cc0f630be83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CatFeat(X):\n",
    "    le = LabelEncoder()\n",
    "    for col in X.columns.tolist():\n",
    "        if (X[col].dtype == 'O'):\n",
    "            X[col] = le.fit_transform(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "5865cf5d-be10-475b-b02a-cf747bb49dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatFeat(X_train)\n",
    "CatFeat(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "198ead74-49aa-4707-b1ad-8fb81d6c4b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "MSSubClass         int64\n",
       "MSZoning           int64\n",
       "LotArea            int64\n",
       "Street             int64\n",
       "                  ...   \n",
       "MoSold             int64\n",
       "YrSold             int64\n",
       "SaleType           int64\n",
       "SaleCondition      int64\n",
       "SalePrice        float64\n",
       "Length: 74, dtype: object"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14f4d3-2c99-4f09-9602-ca08d7383667",
   "metadata": {},
   "source": [
    "# **Обучение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "2e645d37-f628-40bd-9b90-1229098d4f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "xgb.XGBRegressor(\n",
       "    *,\n",
       "    objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = \u001b[33m'reg:squarederror'\u001b[39m,\n",
       "    **kwargs: Any,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Implementation of the scikit-learn API for XGBoost regression.\n",
       "See :doc:`/python/sklearn_estimator` for more information.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "    n_estimators : typing.Optional[int]\n",
       "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
       "        rounds.\n",
       "\n",
       "    max_depth :  typing.Optional[int]\n",
       "\n",
       "        Maximum tree depth for base learners.\n",
       "\n",
       "    max_leaves : typing.Optional[int]\n",
       "\n",
       "        Maximum number of leaves; 0 indicates no limit.\n",
       "\n",
       "    max_bin : typing.Optional[int]\n",
       "\n",
       "        If using histogram-based algorithm, maximum number of bins per feature\n",
       "\n",
       "    grow_policy : typing.Optional[str]\n",
       "\n",
       "        Tree growing policy.\n",
       "\n",
       "        - depthwise: Favors splitting at nodes closest to the node,\n",
       "        - lossguide: Favors splitting at nodes with highest loss change.\n",
       "\n",
       "    learning_rate : typing.Optional[float]\n",
       "\n",
       "        Boosting learning rate (xgb's \"eta\")\n",
       "\n",
       "    verbosity : typing.Optional[int]\n",
       "\n",
       "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "\n",
       "    objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
       "\n",
       "        Specify the learning task and the corresponding learning objective or a custom\n",
       "        objective function to be used.\n",
       "\n",
       "        For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
       "        :ref:`custom-obj-metric` for more information, along with the end note for\n",
       "        function signatures.\n",
       "\n",
       "    booster: typing.Optional[str]\n",
       "\n",
       "        Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
       "\n",
       "    tree_method : typing.Optional[str]\n",
       "\n",
       "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
       "        default, XGBoost will choose the most conservative option available.  It's\n",
       "        recommended to study this option from the parameters document :doc:`tree method\n",
       "        </treemethod>`\n",
       "\n",
       "    n_jobs : typing.Optional[int]\n",
       "\n",
       "        Number of parallel threads used to run xgboost.  When used with other\n",
       "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
       "        parallelize and balance the threads.  Creating thread contention will\n",
       "        significantly slow down both algorithms.\n",
       "\n",
       "    gamma : typing.Optional[float]\n",
       "\n",
       "        (min_split_loss) Minimum loss reduction required to make a further partition on\n",
       "        a leaf node of the tree.\n",
       "\n",
       "    min_child_weight : typing.Optional[float]\n",
       "\n",
       "        Minimum sum of instance weight(hessian) needed in a child.\n",
       "\n",
       "    max_delta_step : typing.Optional[float]\n",
       "\n",
       "        Maximum delta step we allow each tree's weight estimation to be.\n",
       "\n",
       "    subsample : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of the training instance.\n",
       "\n",
       "    sampling_method : typing.Optional[str]\n",
       "\n",
       "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
       "\n",
       "        - ``uniform``: Select random training instances uniformly.\n",
       "        - ``gradient_based``: Select random training instances with higher probability\n",
       "            when the gradient and hessian are larger. (cf. CatBoost)\n",
       "\n",
       "    colsample_bytree : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of columns when constructing each tree.\n",
       "\n",
       "    colsample_bylevel : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of columns for each level.\n",
       "\n",
       "    colsample_bynode : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of columns for each split.\n",
       "\n",
       "    reg_alpha : typing.Optional[float]\n",
       "\n",
       "        L1 regularization term on weights (xgb's alpha).\n",
       "\n",
       "    reg_lambda : typing.Optional[float]\n",
       "\n",
       "        L2 regularization term on weights (xgb's lambda).\n",
       "\n",
       "    scale_pos_weight : typing.Optional[float]\n",
       "        Balancing of positive and negative weights.\n",
       "\n",
       "    base_score : typing.Optional[float]\n",
       "\n",
       "        The initial prediction score of all instances, global bias.\n",
       "\n",
       "    random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
       "\n",
       "        Random number seed.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           Using gblinear booster with shotgun updater is nondeterministic as\n",
       "           it uses Hogwild algorithm.\n",
       "\n",
       "    missing : float\n",
       "\n",
       "        Value in the data which needs to be present as a missing value. Default to\n",
       "        :py:data:`numpy.nan`.\n",
       "\n",
       "    num_parallel_tree: typing.Optional[int]\n",
       "\n",
       "        Used for boosting random forest.\n",
       "\n",
       "    monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
       "\n",
       "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
       "        for more information.\n",
       "\n",
       "    interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
       "\n",
       "        Constraints for interaction representing permitted interactions.  The\n",
       "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
       "        3, 4]]``, where each inner list is a group of indices of features that are\n",
       "        allowed to interact with each other.  See :doc:`tutorial\n",
       "        </tutorials/feature_interaction_constraint>` for more information\n",
       "\n",
       "    importance_type: typing.Optional[str]\n",
       "\n",
       "        The feature importance type for the feature_importances\\_ property:\n",
       "\n",
       "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
       "          \"total_cover\".\n",
       "        * For linear model, only \"weight\" is defined and it's the normalized\n",
       "          coefficients without bias.\n",
       "\n",
       "    device : typing.Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
       "\n",
       "    validate_parameters : typing.Optional[bool]\n",
       "\n",
       "        Give warnings for unknown parameter.\n",
       "\n",
       "    enable_categorical : bool\n",
       "\n",
       "        See the same parameter of :py:class:`DMatrix` for details.\n",
       "\n",
       "    feature_types : typing.Optional[typing.Sequence[str]]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        Used for specifying feature types without constructing a dataframe. See\n",
       "        :py:class:`DMatrix` for details.\n",
       "\n",
       "    feature_weights : Optional[ArrayLike]\n",
       "\n",
       "        Weight for each feature, defines the probability of each feature being selected\n",
       "        when colsample is being used.  All values must be greater than 0, otherwise a\n",
       "        `ValueError` is thrown.\n",
       "\n",
       "    max_cat_to_onehot : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
       "        for categorical data.  When number of categories is lesser than the threshold\n",
       "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
       "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
       "        categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    max_cat_threshold : typing.Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Maximum number of categories considered for each split. Used only by\n",
       "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
       "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    multi_strategy : typing.Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        .. note:: This parameter is working-in-progress.\n",
       "\n",
       "        The strategy used for training multi-target models, including multi-target\n",
       "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
       "        more information.\n",
       "\n",
       "        - ``one_output_per_tree``: One model for each target.\n",
       "        - ``multi_output_tree``:  Use multi-target trees.\n",
       "\n",
       "    eval_metric : typing.Union[str, typing.List[str], typing.Callable, NoneType]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        Metric used for monitoring the training result and early stopping.  It can be a\n",
       "        string or list of strings as names of predefined metric in XGBoost (See\n",
       "        :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
       "        other user defined metric that looks like `sklearn.metrics`.\n",
       "\n",
       "        If custom objective is also provided, then custom metric should implement the\n",
       "        corresponding reverse link function.\n",
       "\n",
       "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
       "        object is provided, it's assumed to be a cost function and by default XGBoost\n",
       "        will minimize the result during early stopping.\n",
       "\n",
       "        For advanced usage on Early stopping like directly choosing to maximize instead\n",
       "        of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
       "        information.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from sklearn.datasets import load_diabetes\n",
       "            from sklearn.metrics import mean_absolute_error\n",
       "            X, y = load_diabetes(return_X_y=True)\n",
       "            reg = xgb.XGBRegressor(\n",
       "                tree_method=\"hist\",\n",
       "                eval_metric=mean_absolute_error,\n",
       "            )\n",
       "            reg.fit(X, y, eval_set=[(X, y)])\n",
       "\n",
       "    early_stopping_rounds : typing.Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        - Activates early stopping. Validation metric needs to improve at least once in\n",
       "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
       "          least one item in **eval_set** in :py:meth:`fit`.\n",
       "\n",
       "        - If early stopping occurs, the model will have two additional attributes:\n",
       "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
       "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
       "          number of trees during inference. If users want to access the full model\n",
       "          (including trees built after early stopping), they can specify the\n",
       "          `iteration_range` in these inference methods. In addition, other utilities\n",
       "          like model plotting can also use the entire model.\n",
       "\n",
       "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
       "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
       "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
       "          metric will be used for early stopping.\n",
       "\n",
       "    callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
       "\n",
       "        List of callback functions that are applied at end of each iteration.\n",
       "        It is possible to use predefined callbacks by using\n",
       "        :ref:`Callback API <callback_api>`.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           States in callback are not preserved during training, which means callback\n",
       "           objects can not be reused for multiple training sessions without\n",
       "           reinitialization or deepcopy.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            for params in parameters_grid:\n",
       "                # be sure to (re)initialize the callbacks before each run\n",
       "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
       "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
       "                reg.fit(X, y)\n",
       "\n",
       "    kwargs : typing.Optional[typing.Any]\n",
       "\n",
       "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
       "        can be found :doc:`here </parameter>`.\n",
       "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
       "        dict simultaneously will result in a TypeError.\n",
       "\n",
       "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
       "            that parameters passed via this argument will interact properly\n",
       "            with scikit-learn.\n",
       "\n",
       "        .. note::  Custom objective function\n",
       "\n",
       "            A custom objective function can be provided for the ``objective``\n",
       "            parameter. In this case, it should have the signature ``objective(y_true,\n",
       "            y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
       "            -> [grad, hess]``:\n",
       "\n",
       "            y_true: array_like of shape [n_samples]\n",
       "                The target values\n",
       "            y_pred: array_like of shape [n_samples]\n",
       "                The predicted values\n",
       "            sample_weight :\n",
       "                Optional sample weights.\n",
       "\n",
       "            grad: array_like of shape [n_samples]\n",
       "                The value of the gradient for each sample point.\n",
       "            hess: array_like of shape [n_samples]\n",
       "                The value of the second derivative for each sample point\n",
       "\n",
       "            Note that, if the custom objective produces negative values for\n",
       "            the Hessian, these will be clipped. If the objective is non-convex,\n",
       "            one might also consider using the expected Hessian (Fisher\n",
       "            information) instead.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/xgboost/sklearn.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     XGBRFRegressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?xgb.XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "7c9b2363-b6b3-470f-9157-b25de8add29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "lgb.LGBMRegressor(\n",
       "    *,\n",
       "    boosting_type: str = \u001b[33m'gbdt'\u001b[39m,\n",
       "    num_leaves: int = \u001b[32m31\u001b[39m,\n",
       "    max_depth: int = -\u001b[32m1\u001b[39m,\n",
       "    learning_rate: float = \u001b[32m0.1\u001b[39m,\n",
       "    n_estimators: int = \u001b[32m100\u001b[39m,\n",
       "    subsample_for_bin: int = \u001b[32m200000\u001b[39m,\n",
       "    objective: Union[str, Callable[[Optional[numpy.ndarray], numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    class_weight: Union[Dict, str, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_split_gain: float = \u001b[32m0.0\u001b[39m,\n",
       "    min_child_weight: float = \u001b[32m0.001\u001b[39m,\n",
       "    min_child_samples: int = \u001b[32m20\u001b[39m,\n",
       "    subsample: float = \u001b[32m1.0\u001b[39m,\n",
       "    subsample_freq: int = \u001b[32m0\u001b[39m,\n",
       "    colsample_bytree: float = \u001b[32m1.0\u001b[39m,\n",
       "    reg_alpha: float = \u001b[32m0.0\u001b[39m,\n",
       "    reg_lambda: float = \u001b[32m0.0\u001b[39m,\n",
       "    random_state: Union[int, numpy.random.mtrand.RandomState, numpy.random._generator.Generator, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    n_jobs: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    importance_type: str = \u001b[33m'split'\u001b[39m,\n",
       "    **kwargs: Any,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m      LightGBM regressor.\n",
       "\u001b[31mInit docstring:\u001b[39m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "    If setting this to a positive value, consider also changing ``num_leaves`` to ``<= 2^max_depth``.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (Hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState or Generator object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    Number of parallel threads to use for training (can be changed at prediction time by\n",
       "    passing it as an extra keyword argument).\n",
       "\n",
       "    For better performance, it is recommended to set this to the number of physical cores\n",
       "    in the CPU.\n",
       "\n",
       "    Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like\n",
       "    scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of\n",
       "    threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds\n",
       "    to using the number of physical cores in the system (its correct detection requires\n",
       "    either the ``joblib`` or the ``psutil`` util libraries to be installed).\n",
       "\n",
       "    .. versionchanged:: 4.0.0\n",
       "\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess``,\n",
       "``objective(y_true, y_pred, weight) -> grad, hess``\n",
       "or ``objective(y_true, y_pred, weight, group) -> grad, hess``:\n",
       "\n",
       "    y_true : numpy 1-D array of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    weight : numpy 1-D array of shape = [n_samples]\n",
       "        The weight of samples. Weights should be non-negative.\n",
       "    group : numpy 1-D array\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes],\n",
       "and grad and hess should be returned in the same format.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/lightgbm/sklearn.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     DaskLGBMRegressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lgb.LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "e7ed9540-4f7a-4a99-8387-03b394bb164a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "CatBoostRegressor(\n",
       "    iterations=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    learning_rate=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    l2_leaf_reg=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    model_size_reg=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    rsm=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    loss_function=\u001b[33m'RMSE'\u001b[39m,\n",
       "    border_count=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    feature_border_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    per_float_feature_quantization=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    input_borders=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    output_borders=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    fold_permutation_block=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    od_pval=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    od_wait=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    od_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    nan_mode=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    counter_calc_method=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    leaf_estimation_iterations=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    leaf_estimation_method=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    thread_count=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_seed=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    use_best_model=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    best_model_min_trees=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    verbose=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    silent=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    logging_level=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    metric_period=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_leaf_count_limit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    store_all_simple_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_ctr_complexity=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    has_time=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    allow_const_label=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    target_border=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    one_hot_max_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_strength=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_score_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    name=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ignored_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    train_dir=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    custom_metric=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eval_metric=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    bagging_temperature=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    save_snapshot=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    snapshot_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    snapshot_interval=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    fold_len_multiplier=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    used_ram_limit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    gpu_ram_part=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    pinned_memory_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    allow_writing_files=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    final_ctr_computation_mode=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    approx_on_full_history=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    boosting_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    simple_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    combinations_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    per_feature_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_description=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_target_border_count=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    task_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    device_config=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    devices=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    bootstrap_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    subsample=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    mvs_reg=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    sampling_frequency=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    sampling_unit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dev_score_calc_obj_block_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dev_efb_max_buckets=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    sparse_features_conflict_fraction=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    n_estimators=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    num_boost_round=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    num_trees=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    colsample_bylevel=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    reg_lambda=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    objective=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eta=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_bin=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    gpu_cat_features_storage=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    data_partition=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    metadata=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    early_stopping_rounds=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    cat_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    grow_policy=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_data_in_leaf=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_child_samples=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_leaves=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    num_leaves=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    score_function=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    leaf_estimation_backtracking=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_history_unit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    monotone_constraints=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    feature_weights=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    penalties_coefficient=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    first_feature_use_penalties=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    per_object_feature_penalties=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    model_shrink_rate=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    model_shrink_mode=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    langevin=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    diffusion_temperature=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    posterior_sampling=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    boost_from_average=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    text_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    tokenizers=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dictionaries=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    feature_calcers=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    text_processing=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    embedding_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eval_fraction=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    fixed_binary_splits=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Implementation of the scikit-learn API for CatBoost regression.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "Like in CatBoostClassifier, except loss_function, classes_count, class_names and class_weights\n",
       "\n",
       "loss_function : string, [default='RMSE']\n",
       "    'RMSE'\n",
       "    'MAE'\n",
       "    'Quantile:alpha=value'\n",
       "    'LogLinQuantile:alpha=value'\n",
       "    'Poisson'\n",
       "    'MAPE'\n",
       "    'Lq:q=value'\n",
       "    'SurvivalAft:dist=value;scale=value'\n",
       "\u001b[31mInit docstring:\u001b[39m\n",
       "Initialize the CatBoost.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "params : dict\n",
       "    Parameters for CatBoost.\n",
       "    If  None, all params are set to their defaults.\n",
       "    If  dict, overriding parameters present in dict.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/catboost/core.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "28e3b992-5cab-42f3-869f-a154c8b1a7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "GradientBoostingRegressor(\n",
       "    *,\n",
       "    loss=\u001b[33m'squared_error'\u001b[39m,\n",
       "    learning_rate=\u001b[32m0.1\u001b[39m,\n",
       "    n_estimators=\u001b[32m100\u001b[39m,\n",
       "    subsample=\u001b[32m1.0\u001b[39m,\n",
       "    criterion=\u001b[33m'friedman_mse'\u001b[39m,\n",
       "    min_samples_split=\u001b[32m2\u001b[39m,\n",
       "    min_samples_leaf=\u001b[32m1\u001b[39m,\n",
       "    min_weight_fraction_leaf=\u001b[32m0.0\u001b[39m,\n",
       "    max_depth=\u001b[32m3\u001b[39m,\n",
       "    min_impurity_decrease=\u001b[32m0.0\u001b[39m,\n",
       "    init=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    alpha=\u001b[32m0.9\u001b[39m,\n",
       "    verbose=\u001b[32m0\u001b[39m,\n",
       "    max_leaf_nodes=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    validation_fraction=\u001b[32m0.1\u001b[39m,\n",
       "    n_iter_no_change=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    tol=\u001b[32m0.0001\u001b[39m,\n",
       "    ccp_alpha=\u001b[32m0.0\u001b[39m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Gradient Boosting for regression.\n",
       "\n",
       "This estimator builds an additive model in a forward stage-wise fashion; it\n",
       "allows for the optimization of arbitrary differentiable loss functions. In\n",
       "each stage a regression tree is fit on the negative gradient of the given\n",
       "loss function.\n",
       "\n",
       ":class:`~sklearn.ensemble.HistGradientBoostingRegressor` is a much faster variant\n",
       "of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and\n",
       "supports monotonic constraints.\n",
       "\n",
       "Read more in the :ref:`User Guide <gradient_boosting>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'\n",
       "    Loss function to be optimized. 'squared_error' refers to the squared\n",
       "    error for regression. 'absolute_error' refers to the absolute error of\n",
       "    regression and is a robust loss function. 'huber' is a\n",
       "    combination of the two. 'quantile' allows quantile regression (use\n",
       "    `alpha` to specify the quantile).\n",
       "    See\n",
       "    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`\n",
       "    for an example that demonstrates quantile regression for creating\n",
       "    prediction intervals with `loss='quantile'`.\n",
       "\n",
       "learning_rate : float, default=0.1\n",
       "    Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
       "    There is a trade-off between learning_rate and n_estimators.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "n_estimators : int, default=100\n",
       "    The number of boosting stages to perform. Gradient boosting\n",
       "    is fairly robust to over-fitting so a large number usually\n",
       "    results in better performance.\n",
       "    Values must be in the range `[1, inf)`.\n",
       "\n",
       "subsample : float, default=1.0\n",
       "    The fraction of samples to be used for fitting the individual base\n",
       "    learners. If smaller than 1.0 this results in Stochastic Gradient\n",
       "    Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
       "    Choosing `subsample < 1.0` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "    Values must be in the range `(0.0, 1.0]`.\n",
       "\n",
       "criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"friedman_mse\" for the mean squared error with improvement score by\n",
       "    Friedman, \"squared_error\" for mean squared error. The default value of\n",
       "    \"friedman_mse\" is generally the best as it can provide a better\n",
       "    approximation in some cases.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, values must be in the range `[2, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n",
       "      will be `ceil(min_samples_split * n_samples)`.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, values must be in the range `[1, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n",
       "      will be `ceil(min_samples_leaf * n_samples)`.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "    Values must be in the range `[0.0, 0.5]`.\n",
       "\n",
       "max_depth : int or None, default=3\n",
       "    Maximum depth of the individual regression estimators. The maximum\n",
       "    depth limits the number of nodes in the tree. Tune this parameter\n",
       "    for best performance; the best value depends on the interaction\n",
       "    of the input variables. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "    If int, values must be in the range `[1, inf)`.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "init : estimator or 'zero', default=None\n",
       "    An estimator object that is used to compute the initial predictions.\n",
       "    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
       "    initial raw predictions are set to zero. By default a\n",
       "    ``DummyEstimator`` is used, predicting either the average target value\n",
       "    (for loss='squared_error'), or a quantile for the other losses.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the random seed given to each Tree estimator at each\n",
       "    boosting iteration.\n",
       "    In addition, it controls the random permutation of the features at\n",
       "    each split (see Notes for more details).\n",
       "    It also controls the random splitting of the training data to obtain a\n",
       "    validation set if `n_iter_no_change` is not None.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "max_features : {'sqrt', 'log2'}, int or float, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, values must be in the range `[1, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0]` and the features\n",
       "      considered at each split will be `max(1, int(max_features * n_features_in_))`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Choosing `max_features < n_features` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "alpha : float, default=0.9\n",
       "    The alpha-quantile of the huber loss function and the quantile\n",
       "    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
       "    Values must be in the range `(0.0, 1.0)`.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Enable verbose output. If 1 then it prints progress and performance\n",
       "    once in a while (the more trees the lower the frequency). If greater\n",
       "    than 1 then it prints progress and performance for every tree.\n",
       "    Values must be in the range `[0, inf)`.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    Values must be in the range `[2, inf)`.\n",
       "    If None, then unlimited number of leaf nodes.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Values must be in the range `(0.0, 1.0)`.\n",
       "    Only used if ``n_iter_no_change`` is set to an integer.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "n_iter_no_change : int, default=None\n",
       "    ``n_iter_no_change`` is used to decide if early stopping will be used\n",
       "    to terminate training when validation score is not improving. By\n",
       "    default it is set to None to disable early stopping. If set to a\n",
       "    number, it will set aside ``validation_fraction`` size of the training\n",
       "    data as validation and terminate training when validation score is not\n",
       "    improving in all of the previous ``n_iter_no_change`` numbers of\n",
       "    iterations.\n",
       "    Values must be in the range `[1, inf)`.\n",
       "    See\n",
       "    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the early stopping. When the loss is not improving\n",
       "    by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
       "    number), the training stops.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "    See :ref:`minimal_cost_complexity_pruning` for details. See\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
       "    for an example of such pruning.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "n_estimators_ : int\n",
       "    The number of estimators as selected by early stopping (if\n",
       "    ``n_iter_no_change`` is specified). Otherwise it is set to\n",
       "    ``n_estimators``.\n",
       "\n",
       "n_trees_per_iteration_ : int\n",
       "    The number of trees that are built at each iteration. For regressors, this is\n",
       "    always 1.\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_improvement_ : ndarray of shape (n_estimators,)\n",
       "    The improvement in loss on the out-of-bag samples\n",
       "    relative to the previous iteration.\n",
       "    ``oob_improvement_[0]`` is the improvement in\n",
       "    loss of the first stage over the ``init`` estimator.\n",
       "    Only available if ``subsample < 1.0``.\n",
       "\n",
       "oob_scores_ : ndarray of shape (n_estimators,)\n",
       "    The full history of the loss values on the out-of-bag\n",
       "    samples. Only available if `subsample < 1.0`.\n",
       "\n",
       "    .. versionadded:: 1.3\n",
       "\n",
       "oob_score_ : float\n",
       "    The last value of the loss on the out-of-bag samples. It is\n",
       "    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n",
       "\n",
       "    .. versionadded:: 1.3\n",
       "\n",
       "train_score_ : ndarray of shape (n_estimators,)\n",
       "    The i-th score ``train_score_[i]`` is the loss of the\n",
       "    model at iteration ``i`` on the in-bag sample.\n",
       "    If ``subsample == 1`` this is the loss on the training data.\n",
       "\n",
       "init_ : estimator\n",
       "    The estimator that provides the initial predictions. Set via the ``init``\n",
       "    argument.\n",
       "\n",
       "estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n",
       "    Classification Tree.\n",
       "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
       "sklearn.ensemble.RandomForestRegressor : A random forest regressor.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data and\n",
       "``max_features=n_features``, if the improvement of the criterion is\n",
       "identical for several splits enumerated during the search of the best\n",
       "split. To obtain a deterministic behaviour during fitting,\n",
       "``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       "J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
       "Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
       "\n",
       "J. Friedman, Stochastic Gradient Boosting, 1999\n",
       "\n",
       "T. Hastie, R. Tibshirani and J. Friedman.\n",
       "Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = make_regression(random_state=0)\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, random_state=0)\n",
       ">>> reg = GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.fit(X_train, y_train)\n",
       "GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.predict(X_test[1:2])\n",
       "array([-61...])\n",
       ">>> reg.score(X_test, y_test)\n",
       "0.4...\n",
       "\n",
       "For a detailed example of utilizing\n",
       ":class:`~sklearn.ensemble.GradientBoostingRegressor`\n",
       "to fit an ensemble of weak predictive models, please refer to\n",
       ":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/sklearn/ensemble/_gb.py\n",
       "\u001b[31mType:\u001b[39m           ABCMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "950b5b0d-76f0-4024-871f-ee328bfb780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressors(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1),\n",
    "        'LightGBM': lgb.LGBMRegressor(n_estimators=100, max_depth=3, learning_rate=0.1),\n",
    "        'CatBoost': CatBoostRegressor(iterations=100, depth=3, learning_rate=0.1, verbose=False),\n",
    "        'Sklearn': GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "    }\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start\n",
    "        preds = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        results[name] = {'MSE': mse, 'time': train_time}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "130a469d-cb82-45a5-85ec-baedd3a238e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3779\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 70\n",
      "[LightGBM] [Info] Start training from score 180921.195890\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "results = train_regressors(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "c0c05ff0-a665-4365-b738-30a613b9855a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Создаем DataFrame с результатами\n",
    "df_regress = pd.DataFrame(results).T\n",
    "\n",
    "# Графики точности и времени обучения\n",
    "fig, axes = plt.subplots(2, figsize=(14, 10))\n",
    "\n",
    "# Регрессия\n",
    "df_regress['MSE'].plot(kind='bar', ax=axes[0], title='MSE (Regression)', color='salmon')\n",
    "df_regress['time'].plot(kind='bar', ax=axes[1], title='Training Time (Regression)', color='gold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
