{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04352d0-1706-4d50-b038-396e3ea514fb",
   "metadata": {},
   "source": [
    "Link to data: https://www.kaggle.com/code/gusthema/house-prices-prediction-using-tfdf/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "4766f865-603a-4a09-8429-7ddfe5f0322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import time\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a41c2-d764-40fc-bd4b-583ae1958252",
   "metadata": {},
   "source": [
    "# **Считывание данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "2eccb906-4c3d-47ec-8d0a-d34e00d313c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>169277.052498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>187758.393989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>183583.683570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>179317.477511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>150730.079977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>167081.220949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>164788.778231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>219222.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>184924.279659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>187741.866657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  169277.052498\n",
       "1     1462  187758.393989\n",
       "2     1463  183583.683570\n",
       "3     1464  179317.477511\n",
       "4     1465  150730.079977\n",
       "...    ...            ...\n",
       "1454  2915  167081.220949\n",
       "1455  2916  164788.778231\n",
       "1456  2917  219222.423400\n",
       "1457  2918  184924.279659\n",
       "1458  2919  187741.866657\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('dataset/sample_submission.csv')\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f44f61-6895-4fbc-8a28-42b2291b3a02",
   "metadata": {},
   "source": [
    "Сразу можно сказать, что тип задачи: регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423aefe9-868d-4646-ac23-effc4c896312",
   "metadata": {},
   "source": [
    "Файл ниже является переводом файла dataset/data_describtion.txt\n",
    "Он лежит на гите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "66a8be3f-0704-42a2-a064-08bb8c8a0b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSSubClass: Классификация типа жилого объекта\n",
      "\n",
      "20 — Одноэтажный, 1946 г. и новее (все стили)\n",
      "30 — Одноэтажный, 1945 г. и старше\n",
      "40 — Одноэтажный с готовой мансардой\n",
      "45 — Полутораэтажный с незавершённой отделкой\n",
      "50 — Полутораэтажный с завершённой отделкой\n",
      "60 — Двухэтажный, 1946+\n",
      "70 — Двухэтажный, 1945 и старше\n",
      "75 — Два с половиной этажа\n",
      "80 — Раздельный/многоуровневый\n",
      "85 — Дом с разделённым холлом\n",
      "90 — Дуплекс\n",
      "120 — Одноэтажный PUD (квартальная застройка), 1946+\n",
      "150 — Полутораэтажный PUD\n",
      "160 — Двухэтажный PUD, 1946+\n",
      "180 — Многоуровневый PUD\n",
      "190 — Переделанный под две семьи\n",
      "MSZoning: Зонирование\n",
      "\n",
      "A — Сельскохозяйственное\n",
      "C — Коммерческое\n",
      "FV — Жилой комплекс у воды\n",
      "I — Промышленное\n",
      "RH — Жилая зона высокой плотности\n",
      "RL — Жилая зона низкой плотности\n",
      "RP — Жилая зона с парком\n",
      "RM — Жилая зона средней плотности\n",
      "Основные параметры участка:\n",
      "\n",
      "LotFrontage — Примыкание к улице (линейные футы)\n",
      "LotArea — Площадь участка (кв. футы)\n",
      "Street — Тип дороги:\n",
      "Grvl — Грунтовая\n",
      "Pave — Асфальтированная\n",
      "Alley — Переулок:\n",
      "Grvl/Pave/NA (нет доступа)\n",
      "Характеристики участка:\n",
      "\n",
      "LotShape — Форма:\n",
      "Reg (правильная), IR1-IR3 (степень неправильности)\n",
      "LandContour — Рельеф:\n",
      "Lvl (ровный), Bnk (склон), HLS (холмистый), Low (низина)\n",
      "Utilities — Коммуникации:\n",
      "AllPub (полные), NoSewr (без канализации), ELO (только электричество)\n",
      "LandSlope — Уклон:\n",
      "Gtl (пологий), Mod (умеренный), Sev (крутой)\n",
      "Расположение:\n",
      "\n",
      "Neighborhood — 28 районов г. Эймс (например, OldTown — Старый город)\n",
      "Condition1/2 — Близость к:\n",
      "Artery (магистрали), RR (жд-пути), PosN (парки)\n",
      "Характеристики здания:\n",
      "\n",
      "BldgType — Тип:\n",
      "1Fam (отдельный дом), TwnhsE (таунхаус)\n",
      "HouseStyle — Архитектура:\n",
      "1Story (одноэтажный), 2.5Unf (2.5 этажа без отделки)\n",
      "OverallQual/Cond — Общее качество/состояние (шкала 1–10)\n",
      "Конструктивные элементы:\n",
      "\n",
      "RoofStyle — Тип крыши:\n",
      "Gable (двускатная), Hip (вальмовая)\n",
      "Foundation — Фундамент:\n",
      "PConc (монолитный бетон), CBlock (блочный)\n",
      "Exterior — Отделка:\n",
      "VinylSd (виниловый сайдинг), BrkFace (облицовочный кирпич)\n",
      "Подвал (Bsmt):\n",
      "\n",
      "Qual — Высота:\n",
      "Ex (100+ дюймов), NA (нет подвала)\n",
      "FinType — Отделка:\n",
      "GLQ (качественная), Unf (незавершённая)\n",
      "Коммуникации:\n",
      "\n",
      "Heating — Отопление:\n",
      "GasA (газовое), Grav (гравитационное)\n",
      "CentralAir — Кондиционирование:\n",
      "Y/N (да/нет)\n",
      "Electrical — Электрика:\n",
      "SBrkr (автоматы), FuseP (пробки)\n",
      "Площади:\n",
      "\n",
      "GrLivArea — Жилая площадь (кв. футы)\n",
      "GarageArea — Площадь гаража\n",
      "WoodDeckSF — Деревянный настил\n",
      "Продажа:\n",
      "\n",
      "SaleType — Тип сделки:\n",
      "WD (стандартная), New (новостройка)\n",
      "SaleCondition — Условия:\n",
      "Normal (обычная), Abnorml (форс-мажор)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat dataset/data_help.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "28c758cf-d7d9-47e5-a86a-b65f92d44ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       169277.052498\n",
       "1       187758.393989\n",
       "2       183583.683570\n",
       "3       179317.477511\n",
       "4       150730.079977\n",
       "            ...      \n",
       "1454    167081.220949\n",
       "1455    164788.778231\n",
       "1456    219222.423400\n",
       "1457    184924.279659\n",
       "1458    187741.866657\n",
       "Name: SalePrice, Length: 1459, dtype: float64"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Загрузка данных\n",
    "X_train = pd.read_csv('dataset/train.csv')\n",
    "X_test = pd.read_csv('dataset/test.csv')\n",
    "y_train = X_train['SalePrice']\n",
    "\n",
    "#Подготовка y_test\n",
    "y_test = sample['SalePrice']\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "9659d0d5-5f9e-4080-ad05-31c49b3ded54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     588 non-null    object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "ee935f07-66db-42fe-af79-5c10c93a4cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0     1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1     1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2     1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "3     1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
       "4     1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1454  2915         160       RM         21.0     1936   Pave   NaN      Reg   \n",
       "1455  2916         160       RM         21.0     1894   Pave   NaN      Reg   \n",
       "1456  2917          20       RL        160.0    20000   Pave   NaN      Reg   \n",
       "1457  2918          85       RL         62.0    10441   Pave   NaN      Reg   \n",
       "1458  2919          60       RL         74.0     9627   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN        Gar2   12500   \n",
       "2            Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            HLS    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1454         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  MnPrv        Shed     700   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         6   2010        WD         Normal          0  \n",
       "1         6   2010        WD         Normal          0  \n",
       "2         3   2010        WD         Normal          0  \n",
       "3         6   2010        WD         Normal          0  \n",
       "4         1   2010        WD         Normal          0  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1454      6   2006        WD         Normal          0  \n",
       "1455      4   2006        WD        Abnorml          0  \n",
       "1456      9   2006        WD        Abnorml          0  \n",
       "1457      7   2006        WD         Normal          0  \n",
       "1458     11   2006        WD         Normal          0  \n",
       "\n",
       "[1459 rows x 81 columns]"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5367599-581d-4637-86d2-8b3e7216198d",
   "metadata": {},
   "source": [
    "# *Обработка пропусков*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb099d3-04dd-4aad-87f9-321694a02b66",
   "metadata": {},
   "source": [
    "Мысль состоит в том, что если столбец имеет больше чем coeff*len(X_train) пропусков, то его надо удалять.\n",
    "В отсальных же случаях просто заменим пропуски на медианное значение.\n",
    "Также столюцы имеют типы данных либо int64, либо float64, либо Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "2c1eb4ea-8dc8-42d5-9b5c-66c9ae0c4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "97f7d1fb-b332-42c0-8457-8e746fbbbeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configNa(X, coeff):\n",
    "    for col in X.columns.tolist():\n",
    "        quantity_NaN = X[col].isnull().sum()\n",
    "        if (quantity_NaN >= coeff*len(X)):\n",
    "            X.drop(col, axis=1, inplace=True)\n",
    "        elif (quantity_NaN):\n",
    "            if (X[col].dtype != 'O'):\n",
    "                median = X[col].median()\n",
    "            else:\n",
    "                median = X[col].mode().values[0] #криво, знаю\n",
    "            X[col].fillna(value = median, inplace=True)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "74c91a9e-dbb6-4dd6-8562-20b56d0eb40f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               0\n",
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 74, dtype: int64"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configNa(X_train, coeff)\n",
    "configNa(X_test, coeff)\n",
    "X_train.isnull().sum() #Зато теперь нет пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "a87fed94-7eee-4940-bc06-c8b21c4123e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1343d5-55b3-45a3-85cc-906061d1b9ee",
   "metadata": {},
   "source": [
    "# **Обработка категориальных признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "514ea80b-0383-47d9-a0cc-cc0f630be83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CatFeat(X):\n",
    "    le = LabelEncoder()\n",
    "    for col in X.columns.tolist():\n",
    "        if (X[col].dtype == 'O'):\n",
    "            X[col] = le.fit_transform(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "5865cf5d-be10-475b-b02a-cf747bb49dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatFeat(X_train)\n",
    "CatFeat(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "198ead74-49aa-4707-b1ad-8fb81d6c4b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               int64\n",
       "MSSubClass       int64\n",
       "MSZoning         int64\n",
       "LotArea          int64\n",
       "Street           int64\n",
       "                 ...  \n",
       "MoSold           int64\n",
       "YrSold           int64\n",
       "SaleType         int64\n",
       "SaleCondition    int64\n",
       "SalePrice        int64\n",
       "Length: 74, dtype: object"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14f4d3-2c99-4f09-9602-ca08d7383667",
   "metadata": {},
   "source": [
    "# **Обучение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "2e645d37-f628-40bd-9b90-1229098d4f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "xgb.XGBRegressor(\n",
       "    *,\n",
       "    objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = \u001b[33m'reg:squarederror'\u001b[39m,\n",
       "    **kwargs: Any,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Implementation of the scikit-learn API for XGBoost regression.\n",
       "See :doc:`/python/sklearn_estimator` for more information.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "    n_estimators : typing.Optional[int]\n",
       "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
       "        rounds.\n",
       "\n",
       "    max_depth :  typing.Optional[int]\n",
       "\n",
       "        Maximum tree depth for base learners.\n",
       "\n",
       "    max_leaves : typing.Optional[int]\n",
       "\n",
       "        Maximum number of leaves; 0 indicates no limit.\n",
       "\n",
       "    max_bin : typing.Optional[int]\n",
       "\n",
       "        If using histogram-based algorithm, maximum number of bins per feature\n",
       "\n",
       "    grow_policy : typing.Optional[str]\n",
       "\n",
       "        Tree growing policy.\n",
       "\n",
       "        - depthwise: Favors splitting at nodes closest to the node,\n",
       "        - lossguide: Favors splitting at nodes with highest loss change.\n",
       "\n",
       "    learning_rate : typing.Optional[float]\n",
       "\n",
       "        Boosting learning rate (xgb's \"eta\")\n",
       "\n",
       "    verbosity : typing.Optional[int]\n",
       "\n",
       "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "\n",
       "    objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
       "\n",
       "        Specify the learning task and the corresponding learning objective or a custom\n",
       "        objective function to be used.\n",
       "\n",
       "        For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
       "        :ref:`custom-obj-metric` for more information, along with the end note for\n",
       "        function signatures.\n",
       "\n",
       "    booster: typing.Optional[str]\n",
       "\n",
       "        Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
       "\n",
       "    tree_method : typing.Optional[str]\n",
       "\n",
       "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
       "        default, XGBoost will choose the most conservative option available.  It's\n",
       "        recommended to study this option from the parameters document :doc:`tree method\n",
       "        </treemethod>`\n",
       "\n",
       "    n_jobs : typing.Optional[int]\n",
       "\n",
       "        Number of parallel threads used to run xgboost.  When used with other\n",
       "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
       "        parallelize and balance the threads.  Creating thread contention will\n",
       "        significantly slow down both algorithms.\n",
       "\n",
       "    gamma : typing.Optional[float]\n",
       "\n",
       "        (min_split_loss) Minimum loss reduction required to make a further partition on\n",
       "        a leaf node of the tree.\n",
       "\n",
       "    min_child_weight : typing.Optional[float]\n",
       "\n",
       "        Minimum sum of instance weight(hessian) needed in a child.\n",
       "\n",
       "    max_delta_step : typing.Optional[float]\n",
       "\n",
       "        Maximum delta step we allow each tree's weight estimation to be.\n",
       "\n",
       "    subsample : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of the training instance.\n",
       "\n",
       "    sampling_method : typing.Optional[str]\n",
       "\n",
       "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
       "\n",
       "        - ``uniform``: Select random training instances uniformly.\n",
       "        - ``gradient_based``: Select random training instances with higher probability\n",
       "            when the gradient and hessian are larger. (cf. CatBoost)\n",
       "\n",
       "    colsample_bytree : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of columns when constructing each tree.\n",
       "\n",
       "    colsample_bylevel : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of columns for each level.\n",
       "\n",
       "    colsample_bynode : typing.Optional[float]\n",
       "\n",
       "        Subsample ratio of columns for each split.\n",
       "\n",
       "    reg_alpha : typing.Optional[float]\n",
       "\n",
       "        L1 regularization term on weights (xgb's alpha).\n",
       "\n",
       "    reg_lambda : typing.Optional[float]\n",
       "\n",
       "        L2 regularization term on weights (xgb's lambda).\n",
       "\n",
       "    scale_pos_weight : typing.Optional[float]\n",
       "        Balancing of positive and negative weights.\n",
       "\n",
       "    base_score : typing.Optional[float]\n",
       "\n",
       "        The initial prediction score of all instances, global bias.\n",
       "\n",
       "    random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
       "\n",
       "        Random number seed.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           Using gblinear booster with shotgun updater is nondeterministic as\n",
       "           it uses Hogwild algorithm.\n",
       "\n",
       "    missing : float\n",
       "\n",
       "        Value in the data which needs to be present as a missing value. Default to\n",
       "        :py:data:`numpy.nan`.\n",
       "\n",
       "    num_parallel_tree: typing.Optional[int]\n",
       "\n",
       "        Used for boosting random forest.\n",
       "\n",
       "    monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
       "\n",
       "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
       "        for more information.\n",
       "\n",
       "    interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
       "\n",
       "        Constraints for interaction representing permitted interactions.  The\n",
       "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
       "        3, 4]]``, where each inner list is a group of indices of features that are\n",
       "        allowed to interact with each other.  See :doc:`tutorial\n",
       "        </tutorials/feature_interaction_constraint>` for more information\n",
       "\n",
       "    importance_type: typing.Optional[str]\n",
       "\n",
       "        The feature importance type for the feature_importances\\_ property:\n",
       "\n",
       "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
       "          \"total_cover\".\n",
       "        * For linear model, only \"weight\" is defined and it's the normalized\n",
       "          coefficients without bias.\n",
       "\n",
       "    device : typing.Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
       "\n",
       "    validate_parameters : typing.Optional[bool]\n",
       "\n",
       "        Give warnings for unknown parameter.\n",
       "\n",
       "    enable_categorical : bool\n",
       "\n",
       "        See the same parameter of :py:class:`DMatrix` for details.\n",
       "\n",
       "    feature_types : typing.Optional[typing.Sequence[str]]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        Used for specifying feature types without constructing a dataframe. See\n",
       "        :py:class:`DMatrix` for details.\n",
       "\n",
       "    feature_weights : Optional[ArrayLike]\n",
       "\n",
       "        Weight for each feature, defines the probability of each feature being selected\n",
       "        when colsample is being used.  All values must be greater than 0, otherwise a\n",
       "        `ValueError` is thrown.\n",
       "\n",
       "    max_cat_to_onehot : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
       "        for categorical data.  When number of categories is lesser than the threshold\n",
       "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
       "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
       "        categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    max_cat_threshold : typing.Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Maximum number of categories considered for each split. Used only by\n",
       "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
       "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    multi_strategy : typing.Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        .. note:: This parameter is working-in-progress.\n",
       "\n",
       "        The strategy used for training multi-target models, including multi-target\n",
       "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
       "        more information.\n",
       "\n",
       "        - ``one_output_per_tree``: One model for each target.\n",
       "        - ``multi_output_tree``:  Use multi-target trees.\n",
       "\n",
       "    eval_metric : typing.Union[str, typing.List[str], typing.Callable, NoneType]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        Metric used for monitoring the training result and early stopping.  It can be a\n",
       "        string or list of strings as names of predefined metric in XGBoost (See\n",
       "        :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
       "        other user defined metric that looks like `sklearn.metrics`.\n",
       "\n",
       "        If custom objective is also provided, then custom metric should implement the\n",
       "        corresponding reverse link function.\n",
       "\n",
       "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
       "        object is provided, it's assumed to be a cost function and by default XGBoost\n",
       "        will minimize the result during early stopping.\n",
       "\n",
       "        For advanced usage on Early stopping like directly choosing to maximize instead\n",
       "        of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
       "        information.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from sklearn.datasets import load_diabetes\n",
       "            from sklearn.metrics import mean_absolute_error\n",
       "            X, y = load_diabetes(return_X_y=True)\n",
       "            reg = xgb.XGBRegressor(\n",
       "                tree_method=\"hist\",\n",
       "                eval_metric=mean_absolute_error,\n",
       "            )\n",
       "            reg.fit(X, y, eval_set=[(X, y)])\n",
       "\n",
       "    early_stopping_rounds : typing.Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        - Activates early stopping. Validation metric needs to improve at least once in\n",
       "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
       "          least one item in **eval_set** in :py:meth:`fit`.\n",
       "\n",
       "        - If early stopping occurs, the model will have two additional attributes:\n",
       "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
       "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
       "          number of trees during inference. If users want to access the full model\n",
       "          (including trees built after early stopping), they can specify the\n",
       "          `iteration_range` in these inference methods. In addition, other utilities\n",
       "          like model plotting can also use the entire model.\n",
       "\n",
       "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
       "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
       "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
       "          metric will be used for early stopping.\n",
       "\n",
       "    callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
       "\n",
       "        List of callback functions that are applied at end of each iteration.\n",
       "        It is possible to use predefined callbacks by using\n",
       "        :ref:`Callback API <callback_api>`.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           States in callback are not preserved during training, which means callback\n",
       "           objects can not be reused for multiple training sessions without\n",
       "           reinitialization or deepcopy.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            for params in parameters_grid:\n",
       "                # be sure to (re)initialize the callbacks before each run\n",
       "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
       "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
       "                reg.fit(X, y)\n",
       "\n",
       "    kwargs : typing.Optional[typing.Any]\n",
       "\n",
       "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
       "        can be found :doc:`here </parameter>`.\n",
       "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
       "        dict simultaneously will result in a TypeError.\n",
       "\n",
       "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
       "            that parameters passed via this argument will interact properly\n",
       "            with scikit-learn.\n",
       "\n",
       "        .. note::  Custom objective function\n",
       "\n",
       "            A custom objective function can be provided for the ``objective``\n",
       "            parameter. In this case, it should have the signature ``objective(y_true,\n",
       "            y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
       "            -> [grad, hess]``:\n",
       "\n",
       "            y_true: array_like of shape [n_samples]\n",
       "                The target values\n",
       "            y_pred: array_like of shape [n_samples]\n",
       "                The predicted values\n",
       "            sample_weight :\n",
       "                Optional sample weights.\n",
       "\n",
       "            grad: array_like of shape [n_samples]\n",
       "                The value of the gradient for each sample point.\n",
       "            hess: array_like of shape [n_samples]\n",
       "                The value of the second derivative for each sample point\n",
       "\n",
       "            Note that, if the custom objective produces negative values for\n",
       "            the Hessian, these will be clipped. If the objective is non-convex,\n",
       "            one might also consider using the expected Hessian (Fisher\n",
       "            information) instead.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/xgboost/sklearn.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     XGBRFRegressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?xgb.XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "7c9b2363-b6b3-470f-9157-b25de8add29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "lgb.LGBMRegressor(\n",
       "    *,\n",
       "    boosting_type: str = \u001b[33m'gbdt'\u001b[39m,\n",
       "    num_leaves: int = \u001b[32m31\u001b[39m,\n",
       "    max_depth: int = -\u001b[32m1\u001b[39m,\n",
       "    learning_rate: float = \u001b[32m0.1\u001b[39m,\n",
       "    n_estimators: int = \u001b[32m100\u001b[39m,\n",
       "    subsample_for_bin: int = \u001b[32m200000\u001b[39m,\n",
       "    objective: Union[str, Callable[[Optional[numpy.ndarray], numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    class_weight: Union[Dict, str, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_split_gain: float = \u001b[32m0.0\u001b[39m,\n",
       "    min_child_weight: float = \u001b[32m0.001\u001b[39m,\n",
       "    min_child_samples: int = \u001b[32m20\u001b[39m,\n",
       "    subsample: float = \u001b[32m1.0\u001b[39m,\n",
       "    subsample_freq: int = \u001b[32m0\u001b[39m,\n",
       "    colsample_bytree: float = \u001b[32m1.0\u001b[39m,\n",
       "    reg_alpha: float = \u001b[32m0.0\u001b[39m,\n",
       "    reg_lambda: float = \u001b[32m0.0\u001b[39m,\n",
       "    random_state: Union[int, numpy.random.mtrand.RandomState, numpy.random._generator.Generator, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    n_jobs: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    importance_type: str = \u001b[33m'split'\u001b[39m,\n",
       "    **kwargs: Any,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m      LightGBM regressor.\n",
       "\u001b[31mInit docstring:\u001b[39m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "    If setting this to a positive value, consider also changing ``num_leaves`` to ``<= 2^max_depth``.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (Hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState or Generator object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    Number of parallel threads to use for training (can be changed at prediction time by\n",
       "    passing it as an extra keyword argument).\n",
       "\n",
       "    For better performance, it is recommended to set this to the number of physical cores\n",
       "    in the CPU.\n",
       "\n",
       "    Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like\n",
       "    scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of\n",
       "    threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds\n",
       "    to using the number of physical cores in the system (its correct detection requires\n",
       "    either the ``joblib`` or the ``psutil`` util libraries to be installed).\n",
       "\n",
       "    .. versionchanged:: 4.0.0\n",
       "\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess``,\n",
       "``objective(y_true, y_pred, weight) -> grad, hess``\n",
       "or ``objective(y_true, y_pred, weight, group) -> grad, hess``:\n",
       "\n",
       "    y_true : numpy 1-D array of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    weight : numpy 1-D array of shape = [n_samples]\n",
       "        The weight of samples. Weights should be non-negative.\n",
       "    group : numpy 1-D array\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes],\n",
       "and grad and hess should be returned in the same format.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/lightgbm/sklearn.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     DaskLGBMRegressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lgb.LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "e7ed9540-4f7a-4a99-8387-03b394bb164a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "CatBoostRegressor(\n",
       "    iterations=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    learning_rate=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    l2_leaf_reg=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    model_size_reg=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    rsm=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    loss_function=\u001b[33m'RMSE'\u001b[39m,\n",
       "    border_count=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    feature_border_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    per_float_feature_quantization=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    input_borders=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    output_borders=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    fold_permutation_block=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    od_pval=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    od_wait=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    od_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    nan_mode=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    counter_calc_method=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    leaf_estimation_iterations=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    leaf_estimation_method=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    thread_count=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_seed=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    use_best_model=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    best_model_min_trees=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    verbose=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    silent=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    logging_level=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    metric_period=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_leaf_count_limit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    store_all_simple_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_ctr_complexity=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    has_time=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    allow_const_label=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    target_border=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    one_hot_max_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_strength=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_score_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    name=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ignored_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    train_dir=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    custom_metric=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eval_metric=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    bagging_temperature=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    save_snapshot=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    snapshot_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    snapshot_interval=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    fold_len_multiplier=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    used_ram_limit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    gpu_ram_part=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    pinned_memory_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    allow_writing_files=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    final_ctr_computation_mode=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    approx_on_full_history=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    boosting_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    simple_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    combinations_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    per_feature_ctr=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_description=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_target_border_count=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    task_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    device_config=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    devices=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    bootstrap_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    subsample=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    mvs_reg=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    sampling_frequency=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    sampling_unit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dev_score_calc_obj_block_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dev_efb_max_buckets=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    sparse_features_conflict_fraction=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    n_estimators=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    num_boost_round=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    num_trees=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    colsample_bylevel=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    reg_lambda=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    objective=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eta=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_bin=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    gpu_cat_features_storage=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    data_partition=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    metadata=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    early_stopping_rounds=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    cat_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    grow_policy=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_data_in_leaf=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_child_samples=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_leaves=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    num_leaves=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    score_function=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    leaf_estimation_backtracking=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ctr_history_unit=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    monotone_constraints=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    feature_weights=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    penalties_coefficient=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    first_feature_use_penalties=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    per_object_feature_penalties=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    model_shrink_rate=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    model_shrink_mode=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    langevin=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    diffusion_temperature=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    posterior_sampling=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    boost_from_average=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    text_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    tokenizers=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dictionaries=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    feature_calcers=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    text_processing=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    embedding_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eval_fraction=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    fixed_binary_splits=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Implementation of the scikit-learn API for CatBoost regression.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "Like in CatBoostClassifier, except loss_function, classes_count, class_names and class_weights\n",
       "\n",
       "loss_function : string, [default='RMSE']\n",
       "    'RMSE'\n",
       "    'MAE'\n",
       "    'Quantile:alpha=value'\n",
       "    'LogLinQuantile:alpha=value'\n",
       "    'Poisson'\n",
       "    'MAPE'\n",
       "    'Lq:q=value'\n",
       "    'SurvivalAft:dist=value;scale=value'\n",
       "\u001b[31mInit docstring:\u001b[39m\n",
       "Initialize the CatBoost.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "params : dict\n",
       "    Parameters for CatBoost.\n",
       "    If  None, all params are set to their defaults.\n",
       "    If  dict, overriding parameters present in dict.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/catboost/core.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "28e3b992-5cab-42f3-869f-a154c8b1a7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "GradientBoostingRegressor(\n",
       "    *,\n",
       "    loss=\u001b[33m'squared_error'\u001b[39m,\n",
       "    learning_rate=\u001b[32m0.1\u001b[39m,\n",
       "    n_estimators=\u001b[32m100\u001b[39m,\n",
       "    subsample=\u001b[32m1.0\u001b[39m,\n",
       "    criterion=\u001b[33m'friedman_mse'\u001b[39m,\n",
       "    min_samples_split=\u001b[32m2\u001b[39m,\n",
       "    min_samples_leaf=\u001b[32m1\u001b[39m,\n",
       "    min_weight_fraction_leaf=\u001b[32m0.0\u001b[39m,\n",
       "    max_depth=\u001b[32m3\u001b[39m,\n",
       "    min_impurity_decrease=\u001b[32m0.0\u001b[39m,\n",
       "    init=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_features=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    alpha=\u001b[32m0.9\u001b[39m,\n",
       "    verbose=\u001b[32m0\u001b[39m,\n",
       "    max_leaf_nodes=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    validation_fraction=\u001b[32m0.1\u001b[39m,\n",
       "    n_iter_no_change=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    tol=\u001b[32m0.0001\u001b[39m,\n",
       "    ccp_alpha=\u001b[32m0.0\u001b[39m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Gradient Boosting for regression.\n",
       "\n",
       "This estimator builds an additive model in a forward stage-wise fashion; it\n",
       "allows for the optimization of arbitrary differentiable loss functions. In\n",
       "each stage a regression tree is fit on the negative gradient of the given\n",
       "loss function.\n",
       "\n",
       ":class:`~sklearn.ensemble.HistGradientBoostingRegressor` is a much faster variant\n",
       "of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and\n",
       "supports monotonic constraints.\n",
       "\n",
       "Read more in the :ref:`User Guide <gradient_boosting>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'\n",
       "    Loss function to be optimized. 'squared_error' refers to the squared\n",
       "    error for regression. 'absolute_error' refers to the absolute error of\n",
       "    regression and is a robust loss function. 'huber' is a\n",
       "    combination of the two. 'quantile' allows quantile regression (use\n",
       "    `alpha` to specify the quantile).\n",
       "    See\n",
       "    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`\n",
       "    for an example that demonstrates quantile regression for creating\n",
       "    prediction intervals with `loss='quantile'`.\n",
       "\n",
       "learning_rate : float, default=0.1\n",
       "    Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
       "    There is a trade-off between learning_rate and n_estimators.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "n_estimators : int, default=100\n",
       "    The number of boosting stages to perform. Gradient boosting\n",
       "    is fairly robust to over-fitting so a large number usually\n",
       "    results in better performance.\n",
       "    Values must be in the range `[1, inf)`.\n",
       "\n",
       "subsample : float, default=1.0\n",
       "    The fraction of samples to be used for fitting the individual base\n",
       "    learners. If smaller than 1.0 this results in Stochastic Gradient\n",
       "    Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
       "    Choosing `subsample < 1.0` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "    Values must be in the range `(0.0, 1.0]`.\n",
       "\n",
       "criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"friedman_mse\" for the mean squared error with improvement score by\n",
       "    Friedman, \"squared_error\" for mean squared error. The default value of\n",
       "    \"friedman_mse\" is generally the best as it can provide a better\n",
       "    approximation in some cases.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, values must be in the range `[2, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n",
       "      will be `ceil(min_samples_split * n_samples)`.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, values must be in the range `[1, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n",
       "      will be `ceil(min_samples_leaf * n_samples)`.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "    Values must be in the range `[0.0, 0.5]`.\n",
       "\n",
       "max_depth : int or None, default=3\n",
       "    Maximum depth of the individual regression estimators. The maximum\n",
       "    depth limits the number of nodes in the tree. Tune this parameter\n",
       "    for best performance; the best value depends on the interaction\n",
       "    of the input variables. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "    If int, values must be in the range `[1, inf)`.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "init : estimator or 'zero', default=None\n",
       "    An estimator object that is used to compute the initial predictions.\n",
       "    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
       "    initial raw predictions are set to zero. By default a\n",
       "    ``DummyEstimator`` is used, predicting either the average target value\n",
       "    (for loss='squared_error'), or a quantile for the other losses.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the random seed given to each Tree estimator at each\n",
       "    boosting iteration.\n",
       "    In addition, it controls the random permutation of the features at\n",
       "    each split (see Notes for more details).\n",
       "    It also controls the random splitting of the training data to obtain a\n",
       "    validation set if `n_iter_no_change` is not None.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "max_features : {'sqrt', 'log2'}, int or float, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, values must be in the range `[1, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0]` and the features\n",
       "      considered at each split will be `max(1, int(max_features * n_features_in_))`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Choosing `max_features < n_features` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "alpha : float, default=0.9\n",
       "    The alpha-quantile of the huber loss function and the quantile\n",
       "    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
       "    Values must be in the range `(0.0, 1.0)`.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Enable verbose output. If 1 then it prints progress and performance\n",
       "    once in a while (the more trees the lower the frequency). If greater\n",
       "    than 1 then it prints progress and performance for every tree.\n",
       "    Values must be in the range `[0, inf)`.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    Values must be in the range `[2, inf)`.\n",
       "    If None, then unlimited number of leaf nodes.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Values must be in the range `(0.0, 1.0)`.\n",
       "    Only used if ``n_iter_no_change`` is set to an integer.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "n_iter_no_change : int, default=None\n",
       "    ``n_iter_no_change`` is used to decide if early stopping will be used\n",
       "    to terminate training when validation score is not improving. By\n",
       "    default it is set to None to disable early stopping. If set to a\n",
       "    number, it will set aside ``validation_fraction`` size of the training\n",
       "    data as validation and terminate training when validation score is not\n",
       "    improving in all of the previous ``n_iter_no_change`` numbers of\n",
       "    iterations.\n",
       "    Values must be in the range `[1, inf)`.\n",
       "    See\n",
       "    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the early stopping. When the loss is not improving\n",
       "    by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
       "    number), the training stops.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "    See :ref:`minimal_cost_complexity_pruning` for details. See\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
       "    for an example of such pruning.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "n_estimators_ : int\n",
       "    The number of estimators as selected by early stopping (if\n",
       "    ``n_iter_no_change`` is specified). Otherwise it is set to\n",
       "    ``n_estimators``.\n",
       "\n",
       "n_trees_per_iteration_ : int\n",
       "    The number of trees that are built at each iteration. For regressors, this is\n",
       "    always 1.\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_improvement_ : ndarray of shape (n_estimators,)\n",
       "    The improvement in loss on the out-of-bag samples\n",
       "    relative to the previous iteration.\n",
       "    ``oob_improvement_[0]`` is the improvement in\n",
       "    loss of the first stage over the ``init`` estimator.\n",
       "    Only available if ``subsample < 1.0``.\n",
       "\n",
       "oob_scores_ : ndarray of shape (n_estimators,)\n",
       "    The full history of the loss values on the out-of-bag\n",
       "    samples. Only available if `subsample < 1.0`.\n",
       "\n",
       "    .. versionadded:: 1.3\n",
       "\n",
       "oob_score_ : float\n",
       "    The last value of the loss on the out-of-bag samples. It is\n",
       "    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n",
       "\n",
       "    .. versionadded:: 1.3\n",
       "\n",
       "train_score_ : ndarray of shape (n_estimators,)\n",
       "    The i-th score ``train_score_[i]`` is the loss of the\n",
       "    model at iteration ``i`` on the in-bag sample.\n",
       "    If ``subsample == 1`` this is the loss on the training data.\n",
       "\n",
       "init_ : estimator\n",
       "    The estimator that provides the initial predictions. Set via the ``init``\n",
       "    argument.\n",
       "\n",
       "estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n",
       "    Classification Tree.\n",
       "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
       "sklearn.ensemble.RandomForestRegressor : A random forest regressor.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data and\n",
       "``max_features=n_features``, if the improvement of the criterion is\n",
       "identical for several splits enumerated during the search of the best\n",
       "split. To obtain a deterministic behaviour during fitting,\n",
       "``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       "J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
       "Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
       "\n",
       "J. Friedman, Stochastic Gradient Boosting, 1999\n",
       "\n",
       "T. Hastie, R. Tibshirani and J. Friedman.\n",
       "Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = make_regression(random_state=0)\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, random_state=0)\n",
       ">>> reg = GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.fit(X_train, y_train)\n",
       "GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.predict(X_test[1:2])\n",
       "array([-61...])\n",
       ">>> reg.score(X_test, y_test)\n",
       "0.4...\n",
       "\n",
       "For a detailed example of utilizing\n",
       ":class:`~sklearn.ensemble.GradientBoostingRegressor`\n",
       "to fit an ensemble of weak predictive models, please refer to\n",
       ":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`.\n",
       "\u001b[31mFile:\u001b[39m           /opt/homebrew/lib/python3.13/site-packages/sklearn/ensemble/_gb.py\n",
       "\u001b[31mType:\u001b[39m           ABCMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "950b5b0d-76f0-4024-871f-ee328bfb780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressors(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1),\n",
    "        'LightGBM': lgb.LGBMRegressor(n_estimators=100, max_depth=3, learning_rate=0.1),\n",
    "        'CatBoost': CatBoostRegressor(iterations=100, depth=3, learning_rate=0.1, verbose=False),\n",
    "        'Sklearn': GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "    }\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start\n",
    "        preds = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        results[name] = {'MSE': mse, 'time': train_time}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "130a469d-cb82-45a5-85ec-baedd3a238e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:MSZoning: object, Street: object, Alley: object, LotShape: object, LandContour: object, Utilities: object, LotConfig: object, LandSlope: object, Neighborhood: object, Condition1: object, Condition2: object, BldgType: object, HouseStyle: object, RoofStyle: object, RoofMatl: object, Exterior1st: object, Exterior2nd: object, MasVnrType: object, ExterQual: object, ExterCond: object, Foundation: object, BsmtQual: object, BsmtCond: object, BsmtExposure: object, BsmtFinType1: object, BsmtFinType2: object, Heating: object, HeatingQC: object, CentralAir: object, Electrical: object, KitchenQual: object, Functional: object, FireplaceQu: object, GarageType: object, GarageFinish: object, GarageQual: object, GarageCond: object, PavedDrive: object, PoolQC: object, Fence: object, MiscFeature: object, SaleType: object, SaleCondition: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/data.py:407\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     new_feature_types.append(\u001b[43m_pandas_dtype_mapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'object'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[531]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mtrain_regressors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[530]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_regressors\u001b[39m\u001b[34m(X_train, X_test, y_train, y_test)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m     10\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     train_time = time.time() - start\n\u001b[32m     13\u001b[39m     preds = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/sklearn.py:1222\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1217\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1218\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1219\u001b[39m )\n\u001b[32m   1221\u001b[39m evals_result: TrainingCallback.EvalsLog = {}\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m train_dmatrix, evals = \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1242\u001b[39m     obj: Optional[Objective] = _objective_decorator(\u001b[38;5;28mself\u001b[39m.objective)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/sklearn.py:628\u001b[39m, in \u001b[36m_wrap_evaluation_matrices\u001b[39m\u001b[34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_evaluation_matrices\u001b[39m(\n\u001b[32m    608\u001b[39m     *,\n\u001b[32m    609\u001b[39m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m     feature_types: Optional[FeatureTypes],\n\u001b[32m    625\u001b[39m ) -> Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m    626\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[33;03m    way.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     train_dmatrix = \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m     n_validation = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) -> Sequence:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/sklearn.py:1137\u001b[39m, in \u001b[36mXGBModel._create_dmatrix\u001b[39m\u001b[34m(self, ref, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m.tree_method, \u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.booster != \u001b[33m\"\u001b[39m\u001b[33mgblinear\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_bin\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[32m   1141\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:1614\u001b[39m, in \u001b[36mQuantileDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[39m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1595\u001b[39m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1607\u001b[39m         )\n\u001b[32m   1608\u001b[39m     ):\n\u001b[32m   1609\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf data iterator is used as input, data like label should be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspecified as batch argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1612\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:1678\u001b[39m, in \u001b[36mQuantileDMatrix._init\u001b[39m\u001b[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[39m\n\u001b[32m   1663\u001b[39m config = make_jcargs(\n\u001b[32m   1664\u001b[39m     nthread=\u001b[38;5;28mself\u001b[39m.nthread,\n\u001b[32m   1665\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1666\u001b[39m     max_bin=\u001b[38;5;28mself\u001b[39m.max_bin,\n\u001b[32m   1667\u001b[39m     max_quantile_blocks=max_quantile_blocks,\n\u001b[32m   1668\u001b[39m )\n\u001b[32m   1669\u001b[39m ret = _LIB.XGQuantileDMatrixCreateFromCallback(\n\u001b[32m   1670\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1671\u001b[39m     it.proxy.handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1676\u001b[39m     ctypes.byref(handle),\n\u001b[32m   1677\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1678\u001b[39m \u001b[43mit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[32m   1680\u001b[39m _check_call(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:572\u001b[39m, in \u001b[36mDataIter.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    570\u001b[39m exc = \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    571\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:553\u001b[39m, in \u001b[36mDataIter._handle_exception\u001b[39m\u001b[34m(self, fn, dft_ret)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[32m    558\u001b[39m     tb = sys.exc_info()[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:640\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m._temporary_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m), \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/data.py:1654\u001b[39m, in \u001b[36mSingleBatchInternalIter.next\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m   1652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28mself\u001b[39m.it += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/core.py:620\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.input_data\u001b[39m\u001b[34m(data, feature_names, feature_types, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m     new, cat_codes, feature_names, feature_types = \u001b[38;5;28mself\u001b[39m._temporary_data\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     new, cat_codes, feature_names, feature_types = \u001b[43m_proxy_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_enable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;28mself\u001b[39m._temporary_data = (new, cat_codes, feature_names, feature_types)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/data.py:1707\u001b[39m, in \u001b[36m_proxy_transform\u001b[39m\u001b[34m(data, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_pa, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[32m   1706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     df, feature_names, feature_types = \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[32m   1711\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mValue type is not supported for data iterator:\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/data.py:640\u001b[39m, in \u001b[36m_transform_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.columns) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _matrix_meta:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot have multiple columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m feature_names, feature_types = \u001b[43mpandas_feature_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m arrays = pandas_transform_data(data)\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/data.py:409\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    407\u001b[39m             new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m             \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     feature_types = new_feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/xgboost/data.py:372\u001b[39m, in \u001b[36m_invalid_dataframe_dtype\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    370\u001b[39m type_err = \u001b[33m\"\u001b[39m\u001b[33mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:MSZoning: object, Street: object, Alley: object, LotShape: object, LandContour: object, Utilities: object, LotConfig: object, LandSlope: object, Neighborhood: object, Condition1: object, Condition2: object, BldgType: object, HouseStyle: object, RoofStyle: object, RoofMatl: object, Exterior1st: object, Exterior2nd: object, MasVnrType: object, ExterQual: object, ExterCond: object, Foundation: object, BsmtQual: object, BsmtCond: object, BsmtExposure: object, BsmtFinType1: object, BsmtFinType2: object, Heating: object, HeatingQC: object, CentralAir: object, Electrical: object, KitchenQual: object, Functional: object, FireplaceQu: object, GarageType: object, GarageFinish: object, GarageQual: object, GarageCond: object, PavedDrive: object, PoolQC: object, Fence: object, MiscFeature: object, SaleType: object, SaleCondition: object"
     ]
    }
   ],
   "source": [
    "results = train_regressors(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "c0c05ff0-a665-4365-b738-30a613b9855a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>1.783670e+09</td>\n",
       "      <td>0.087972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>3.421782e+09</td>\n",
       "      <td>0.047673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>3.357177e+09</td>\n",
       "      <td>0.036550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sklearn</th>\n",
       "      <td>1.339335e+09</td>\n",
       "      <td>0.397223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   MSE      time\n",
       "XGBoost   1.783670e+09  0.087972\n",
       "LightGBM  3.421782e+09  0.047673\n",
       "CatBoost  3.357177e+09  0.036550\n",
       "Sklearn   1.339335e+09  0.397223"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_regress = pd.DataFrame(results).T\n",
    "df_regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "9bf4223d-c29b-464e-b5bb-bbb57a7051c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1454    0\n",
       "1455    0\n",
       "1456    0\n",
       "1457    0\n",
       "1458    0\n",
       "Name: SalePrice, Length: 1459, dtype: int64"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5d722-4141-4fbb-88ea-32328b9d8f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
